{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vIoy1aktWzy4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.1.2-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/Oleg_Litvinov1/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/Oleg_Litvinov1/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBQA2nPoFz6Y"
   },
   "source": [
    "word level https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbb3zpvHHl-O"
   },
   "source": [
    "<div class=\"cite2c-biblio\"></div># Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WrzxG_CmHnf7"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import hashlib\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from IPython.display import Image\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import multiprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oy-27165GM1X"
   },
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Oleg_Litvinov1/Documents/Code/fact-ai'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dtKjfUL2Fcro",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Read train, val, and test sets into string objects\n",
    "train_data = Path('wikitext-103/wiki.train.tokens').read_text()\n",
    "val_data = Path('wikitext-103/wiki.valid.tokens').read_text()\n",
    "test_data = Path('wikitext-103/wiki.test.tokens').read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(raw_str: str):\n",
    "    # Split raw text dataset into individual lines\n",
    "    lines = raw_str.splitlines()\n",
    "\n",
    "    # Lowercase\n",
    "    lower_lines = [line.lower() for line in lines]\n",
    "    \n",
    "    # Remove casing, punctuation, special characters, and stop words and also lemmatize the words on a subset of the first 110 articles in the train data\n",
    "    sentences = [re.sub('[^ a-zA-Z0-9]|unk', '', s) for s in lower_lines]\n",
    "    \n",
    "    sentences = [s for s in sentences if s.strip() != '']\n",
    "    \n",
    "    sentences_filtered = [[w for w in s.split(' ') if w not in stop_words.union({''})] for s in sentences]  # (w for s in lower_lines for w in s.split(' ') if w not in stop_words)\n",
    "    \n",
    "    return sentences_filtered    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 5s, sys: 3.55 s, total: 2min 9s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_sentences = preprocess(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Only use the PoS tagger, or processing will take very long\n",
    "# nlp = spacy.load('en_core_web_sm', disable=[\n",
    "#     'parser',\n",
    "#     'entity',\n",
    "#     'ner',\n",
    "#     'entity_linker',\n",
    "#     'entity_ruler',\n",
    "#     'textcat',\n",
    "#     'textcat_multilabel',\n",
    "#     'morphologizer',\n",
    "#     'senter',\n",
    "#     'sentencizer',\n",
    "#     'tok2vec',\n",
    "#     'transformers'\n",
    "# ])\n",
    "\n",
    "# tsn = [nlp(' '.join(s)) for s in train_sentences[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3xPi0keHD51",
    "outputId": "6c51cd81-efb8-4f10-f154-606283ea92c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 229 ms, total: 12.3 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# import re\n",
    "\n",
    "# # Remove casing, punctuation, special characters, and stop words and also lemmatize the words on a subset of the first 110 articles in the train data\n",
    "# # my_new_text = re.sub('[^ a-zA-Z0-9]|unk', '', train_data[:10000000])\n",
    "# sentences = [re.sub('[^ a-zA-Z0-9]|unk', '', s) for s in sentences]\n",
    "\n",
    "# sentences = [word_tokenize(d.lower()) for s in sentences]\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# filtered_sentence = (w for s in sentences for w in s if w not in stop_words)\n",
    "\n",
    "# lemma = WordNetLemmatizer()\n",
    "# normalized = \" \".join(lemma.lemmatize(word) for word in filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "WslEJ3rmTXhk"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    # sentences=word_tokens,  # a list of lists of tokens\n",
    "                 sg=1,\n",
    "                 negative=5,\n",
    "                 vector_size=100, \n",
    "                 window=5, \n",
    "                 min_count=10, \n",
    "                 workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giq37pBZVSNV",
    "outputId": "402177a2-161b-4cdf-fd38-a22847a8838a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.18 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "model.build_vocab(train_sentences, progress_per=1000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52038651/loss-does-not-decrease-during-training-word2vec-gensim\n",
    "# init callback class\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsO7E-8RZhrJ",
    "outputId": "522a0925-b879-423b-e314-15a967d3d4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 44224868.0\n",
      "Loss after epoch 1: 23500244.0\n",
      "Loss after epoch 2: 2723640.0\n",
      "Loss after epoch 3: 2600856.0\n",
      "Loss after epoch 4: 2449144.0\n",
      "Loss after epoch 5: 2347256.0\n",
      "Loss after epoch 6: 2194144.0\n",
      "Loss after epoch 7: 2098144.0\n",
      "Loss after epoch 8: 2006360.0\n",
      "Loss after epoch 9: 2000000.0\n",
      "Time to train the model: 12.83 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "model.train(train_sentences, \n",
    "            total_examples=model.corpus_count, \n",
    "            epochs=10, \n",
    "            report_delay=1,\n",
    "            compute_loss=True, \n",
    "            callbacks=[callback()])\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "QsYIJCRebK8T"
   },
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "pJ4BzoT7jG7X",
    "outputId": "bb599d26-59e6-4e7b-e6f0-99170f0c40f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('murders', 0.8885542154312134),\n",
       " ('murderer', 0.8399878144264221),\n",
       " ('kidnapping', 0.8296462297439575),\n",
       " ('murdering', 0.8261697292327881),\n",
       " ('convicted', 0.8101226687431335),\n",
       " ('murdered', 0.7853724956512451),\n",
       " ('rape', 0.7829334139823914),\n",
       " ('arrest', 0.7744185328483582),\n",
       " ('robbery', 0.7712156772613525),\n",
       " ('crime', 0.7708020806312561)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"murder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "pJ4BzoT7jG7X",
    "outputId": "bb599d26-59e6-4e7b-e6f0-99170f0c40f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('working', 0.5911438465118408),\n",
       " ('welfare', 0.5858847498893738),\n",
       " ('works', 0.5836688876152039),\n",
       " ('production', 0.5688177347183228),\n",
       " ('prioritisation', 0.5571842193603516),\n",
       " ('craftspeople', 0.5515726804733276),\n",
       " ('domestically', 0.5431331396102905),\n",
       " ('employment', 0.5428930521011353),\n",
       " ('importing', 0.5414029955863953),\n",
       " ('finance', 0.5345153212547302)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"domestic\", \"work\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('working', 0.5911438465118408),\n",
       " ('welfare', 0.5858847498893738),\n",
       " ('works', 0.5836688876152039),\n",
       " ('production', 0.5688177347183228),\n",
       " ('prioritisation', 0.5571842193603516),\n",
       " ('craftspeople', 0.5515726804733276),\n",
       " ('domestically', 0.5431331396102905),\n",
       " ('employment', 0.5428930521011353),\n",
       " ('importing', 0.5414029955863953),\n",
       " ('finance', 0.5345153212547302)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "model_loaded.wv.most_similar(positive=[\"domestic\", \"work\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4rII/yY/P8swdpbZLv8lZ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "12KvkarNL-Z2a7ZLnAp1mSzV2ygG16kbR",
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
